# chat-with-llama2
A simple Python CLI to interact with LLaMA2 locally using Ollama. Enter any prompt and get instant responses from the model â€“ no internet or cloud required.
